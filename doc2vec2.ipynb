{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument, LabeledSentence\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import re\n",
    "import sqlite3\n",
    "\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spltr = re.compile( r'\\W+' )\n",
    "tag_map = []\n",
    "arXiv_start = 0\n",
    "\n",
    "def get_abstracts_indices_from_DBLP():\n",
    "    papers = pd.read_csv( '../data-kw-216432.csv' )\n",
    "    # TODO: Get from whole dataset (too)\n",
    "    for i, abstract in enumerate( papers['ABSTRACT'] ):\n",
    "        index = papers['INDEX'][i]  # NOTE: This is a string too, yeah??\n",
    "        if isinstance( abstract, float ):\n",
    "            #print( 'NO ABSTRACT FOUND at Index = '+ index )\n",
    "            continue\n",
    "        #ab_words = abstract.lower().split( r'\\w+' )  # TODO: CHANGE TO SOMETHING ACTUALLY MEANINGFUL\n",
    "        ab_words = [ w for w in spltr.split( abstract.lower() ) if w != '' ]\n",
    "        #print( ab_words )\n",
    "        tag_map.append( index )\n",
    "        yield TaggedDocument( words=ab_words, tags=[i] )  # NOTE: WAS tags=[i]\n",
    "        #yield LabeledSentence( words=ab_words, tags=[int(index)] )\n",
    "        # TODO: Consider having the list of references as multiple tags/labels?\n",
    "        \n",
    "def get_abstracts_indices_from_arXiv():\n",
    "    arXiv_start = len( tag_map )\n",
    "    conn = sqlite3.connect( '../arxiv-server/paper_trail.db' )\n",
    "    curs = conn.cursor()\n",
    "    for i, row in enumerate( curs.execute( 'SELECT id, abstract FROM abstracts' ) ):\n",
    "        index = row[0]   # NOTE: These are strings, yeah??\n",
    "        abstract = row[1]\n",
    "        ab_words = [ w for w in spltr.split( abstract.lower() ) if w != '' ]\n",
    "        tag_map.append( index )  # NOTE: Diff datatypes cannot be in same list! ?\n",
    "        yield TaggedDocument( words=ab_words, tags=[arXiv_start + i])\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DBLP_docs = list( get_abstracts_indices_from_DBLP() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arXiv_docs = list( get_abstracts_indices_from_arXiv() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = DBLP_docs + arXiv_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_paper_abstract( idx ):\n",
    "    \"\"\" Given a model doc tag, output the respective DBLP or arXiv paper abstract\"\"\"\n",
    "    return ' '.join( docs[idx].words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec( docs, size=400, window=8, min_count=2, workers=cores )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(92, 0.9642688632011414),\n",
       " (257, 0.9634057879447937),\n",
       " (264, 0.9507011771202087),\n",
       " (97, 0.9483972787857056),\n",
       " (186, 0.9463803172111511),\n",
       " (330, 0.9451543092727661),\n",
       " (272, 0.9448680877685547),\n",
       " (280, 0.9366728067398071),\n",
       " (273, 0.9333893060684204),\n",
       " (230, 0.9290798902511597)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.most_similar( 397 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Code you'll find on nycdatascience.com:\n",
    "def cossim( v1, v2 ):\n",
    "    return np.dot(v1, v2) / np.sqrt( np.dot(v1, v1) ) / np.sqrt( np.dot(v2, v2) )\n",
    "def argmaxn( l, n ):\n",
    "    l_copy = list(l)\n",
    "    args = []\n",
    "    for i in range(n):\n",
    "        arg = np.argmax(l_copy)\n",
    "        args.append(arg)\n",
    "        l_copy[arg] = -float('inf')\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST ABSTRACT:\n",
      "in this paper we present our research on social interaction in co located handheld augmented reality ar games these games are characterized by shared physical spaces that promote physical awareness among players and individual gaming devices that support both public and private information one result of our exploration of the design and evaluation of such games is a prototype called bragfish through bragfish we aim to investigate the connections between the observed game experience focusing on social and physical interaction and the designed affordances of our ar handheld game our evaluation of bragfish shows that most of our participants form strategies for social play by leveraging visual aural and physical cues from the shared space moreover we use this as an example to motivate discussions on how to improve social play experiences for co located handheld games by designing for shared spaces\n",
      "\n",
      "  SIMILAR TO:\n",
      "\n",
      "emergent game formats such as machinima that use game worlds as expressive 3d performance spaces have new expressive powers with an increase of the quality of their underlying graphic and animation systems nevertheless they still lack intuitive control mechanisms set direction and acting are limited by tools that were designed to create and play video games rather than produce expressive performance pieces these tools do a poor job of capturing the performative expression that characterizes more mature media such as film tangible interfaces can help open up the game systems for more intuitive character control needed for a greater level of expression in the digital real time world the tui3d project tangible user interfaces for real time 3d addresses production and performative challenges involved in creating machinima through the development of tangible interfaces for controlling 3d virtual actors and environments in real time in this paper we present the design and implementation of a tangible puppet prototype for virtual character control in the unreal game engine and discuss initial user feedback\n",
      "  (id = 92, with a score of 0.9642688632011414)\n",
      "\n",
      "trans reality games are games that take advantage of pervasive mobile ubiquitous location based and mixed reality technical infrastructures to create game spaces that can include physical reality together with one or more virtual realities creating these games requires basic design decisions about the relationships between the large scale game spaces involved in particular the different game spaces can be related by general 3d coordinate system transforms together with decisions regarding isomorphism at different levels of spatial scale the result is a large space of possibilities for trans reality game space design supporting very different forms of game mechanics\n",
      "  (id = 257, with a score of 0.9634057879447937)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_idx = 397\n",
    "compare = 2\n",
    "\n",
    "compare = min( compare, 10 )\n",
    "print( 'TEST ABSTRACT:\\n' + get_paper_abstract(test_idx) )\n",
    "print( '\\n  SIMILAR TO:\\n' )\n",
    "for idx_score in model.docvecs.most_similar( test_idx )[:compare]:\n",
    "    print( get_paper_abstract(idx_score[0]) )\n",
    "    print( '  (id = '+ str(idx_score[0]) + ', with a score of '+ str(idx_score[1]) +')\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172349 0.355598\n",
      "198584 0.337821\n",
      "151634 0.33091\n",
      "169540 0.311499\n",
      "197432 0.308624\n"
     ]
    }
   ],
   "source": [
    "# NOTE: THIS IS WHERE YOU PUT USER INPUT\n",
    "vec = model.infer_vector( ['handheld', 'augmented', 'reality'] )\n",
    "most_sim = 5\n",
    "# Code you'll find on nycdatascience.com:\n",
    "cossims = list( map( lambda v: cossim(vec, v), model.docvecs ) )\n",
    "sim_ids = argmaxn( cossims, most_sim )\n",
    "for i in range(most_sim):\n",
    "    print( sim_ids[i], cossims[sim_ids[i]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code you'll find on that other website:\n",
    "import lda\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# TODO: max_features was initially 10000\n",
    "cvectorizer = CountVectorizer( min_df=4, max_features=5000, stop_words='english' )\n",
    "abstracts = [ ' '.join( x.words ) for x in DBLP_docs ]\n",
    "abstracts += [ ' '.join( x.words ) for x in arXiv_docs ]\n",
    "cvz = cvectorizer.fit_transform( abstracts )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_topics = 20\n",
    "n_iter = 2000\n",
    "lda_model = lda.LDA( n_topics=n_topics, n_iter=n_iter )\n",
    "X_topics = lda_model.fit_transform( cvz )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_top_words = 8\n",
    "topic_summaries = []\n",
    "\n",
    "topic_word = lda_model.topic_word_\n",
    "vocab = cvectorizer.get_feature_names()\n",
    "for i, topic_dist in enumerate( topic_word ):\n",
    "    topic_words = np.array( vocab )[ np.argsort(topic_dist) ][ : -(n_top_words+1):-1 ]\n",
    "    topic_summaries.append( ' '.join( topic_words ) )\n",
    "    print( 'Topic {}: {}'.format(i, ' '.join( topic_words) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4990"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_map[92]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'due to the increase of interest in augmented reality ar the potential uses of ar are increasing also it can benefit the user in various fields such as education business medicine and other augmented reality supports the real environment with synthetic environment to give more details and meaning to the objects in the real word ar refers to a situation in which the goal is to supplement a user s perception of the real world through the addition of virtual objects this paper is an attempt to make a survey of web based augmented reality applications and make a comparison among them'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_paper_abstract( 151634 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('clustering', 0.3813590407371521),\n",
       " ('ranking', 0.37842971086502075),\n",
       " ('ensemble', 0.3661080598831177),\n",
       " ('fusion', 0.3224477767944336),\n",
       " ('classifiers', 0.32240602374076843),\n",
       " ('inference', 0.30264925956726074),\n",
       " ('clusterings', 0.2996658682823181),\n",
       " ('pruning', 0.2980048656463623),\n",
       " ('boosting', 0.2952820360660553),\n",
       " ('encoding', 0.293839693069458)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar( positive=['classification'], negative=['recognition'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15382886432060081"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_similarity( ['human', 'chess'], ['deep', 'blue'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# When checking user's input, (if necessary) see if the keywords are present in the model or not"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
