{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import re\n",
    "import sqlite3\n",
    "\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spltr = re.compile( r'\\W+' )\n",
    "#tag_map = []\n",
    "arXiv_start = 0\n",
    "\n",
    "def get_abstracts_indices_from_DBLP():\n",
    "    papers = pd.read_csv( '../../data-kw.csv' )\n",
    "    # TODO: Get from whole dataset (too)\n",
    "    for i, abstract in enumerate( papers['ABSTRACT'] ):\n",
    "        index = papers['INDEX'][i]  # NOTE: This is a string too, yeah??\n",
    "        if isinstance( abstract, float ):\n",
    "            #print( 'NO ABSTRACT FOUND at Index = '+ index )\n",
    "            continue\n",
    "        ab_words = [ w for w in spltr.split( abstract.lower() ) if w != '' ]\n",
    "        #tag_map.append( index )\n",
    "        yield TaggedDocument( words=ab_words, tags=[int(index)] )  # NOTE: WAS tags=[i]\n",
    "        # TODO: Consider having the list of references as multiple tags/labels?\n",
    "        \n",
    "def get_abstracts_indices_from_arXiv():\n",
    "    arXiv_start = len( tag_map )\n",
    "    conn = sqlite3.connect( '../../arxiv-server/paper_trail.db' )\n",
    "    curs = conn.cursor()\n",
    "    for i, row in enumerate( curs.execute( 'SELECT id, abstract FROM abstracts' ) ):\n",
    "        index = row[0]   # NOTE: These are strings, yeah??\n",
    "        abstract = row[1]\n",
    "        ab_words = [ w for w in spltr.split( abstract.lower() ) if w != '' ]\n",
    "        #tag_map.append( index )\n",
    "        yield TaggedDocument( words=ab_words, tags=[int(index)])  # NOTE: WAS tags=[arXiv_start + i]\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_DBLP_data():\n",
    "    papers = pd.read_csv( '../../data-kw.csv' )\n",
    "    with open( '../../DBLP_ids_abstracts.txt', 'w' ) as f:\n",
    "        for i, abstract in enumerate( papers['ABSTRACT'] ):\n",
    "            index = papers['INDEX'][i]\n",
    "            f.write( str(index) + '|' + str(abstract) + '\\n' )\n",
    "            \n",
    "transform_DBLP_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265964\n",
      "0\n",
      "265964\n"
     ]
    }
   ],
   "source": [
    "DBLP_docs = list( get_abstracts_indices_from_DBLP() )\n",
    "#arXiv_docs = list( get_abstracts_indices_from_arXiv() )\n",
    "arXiv_docs = []\n",
    "docs = DBLP_docs + arXiv_docs\n",
    "print( len(DBLP_docs) )\n",
    "print( len(arXiv_docs) )\n",
    "print( len(docs) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_paper_abstract( idx ):\n",
    "    \"\"\" Given a model doc tag, output the respective DBLP or arXiv paper abstract\"\"\"\n",
    "    return ' '.join( docs[idx].words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec( docs, size=200, window=8, min_count=2, workers=cores )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save( 'doc2vecmodel_s200w8mc2' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = Doc2Vec.load( 'doc2vecmodel_s400w8mc2' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import datetime\n",
    "\n",
    "# From gensim doc:\n",
    "simple_models = [\n",
    "    # PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "    Doc2Vec(dm=1, dm_concat=1, size=400, window=5, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, size=400, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DM w/average\n",
    "    Doc2Vec(dm=1, dm_mean=1, size=400, window=10, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # My others:\n",
    "    Doc2Vec(size=200, workers=cores),\n",
    "    Doc2Vec(size=300, workers=cores),\n",
    "    Doc2Vec(size=400, workers=cores)\n",
    "]\n",
    "\n",
    "simple_models[0].build_vocab(alldocs)  # PV-DM/concat requires one special NULL word so it serves as template\n",
    "for model in simple_models[1:]:\n",
    "    model.reset_from(simple_models[0])\n",
    "    \n",
    "alpha, min_alpha, passes = (0.025, 0.001, 20)\n",
    "alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "for epoch in range(passes):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(363, 0.961861789226532),\n",
       " (92, 0.9612609148025513),\n",
       " (277, 0.956291913986206),\n",
       " (441, 0.9476752877235413),\n",
       " (256, 0.9442665576934814),\n",
       " (186, 0.9399662613868713),\n",
       " (264, 0.933863639831543),\n",
       " (368, 0.9276899695396423),\n",
       " (176, 0.9242762923240662),\n",
       " (291, 0.9103182554244995)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.most_similar( 397 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST ABSTRACT:\n",
      "in this paper we present our research on social interaction in co located handheld augmented reality ar games these games are characterized by shared physical spaces that promote physical awareness among players and individual gaming devices that support both public and private information one result of our exploration of the design and evaluation of such games is a prototype called bragfish through bragfish we aim to investigate the connections between the observed game experience focusing on social and physical interaction and the designed affordances of our ar handheld game our evaluation of bragfish shows that most of our participants form strategies for social play by leveraging visual aural and physical cues from the shared space moreover we use this as an example to motivate discussions on how to improve social play experiences for co located handheld games by designing for shared spaces\n",
      "\n",
      "  SIMILAR TO:\n",
      "\n",
      "avatars are traditionally understood as representing their human counterparts in virtual contexts by incorporating many aspects of a person s real world physical characteristics within the virtual form an alternate approach in which avatars are instead imbued with non human characteristics challenges the limitations of solely anthropomorphic principles and expands the potential of avatars for virtual world interaction and communication this paper provides a brief history of non anthropomorphic avatars with a focus on exploring the current use of such avatars in virtual worlds in order to explain the shift in degree of anthropomorphism we discuss goffman s theory of symbolic interactionism which holds that the self is constructed as a persona through social performance and relates identity to social behavior rather than appearance since non anthropomorphic avatars are persistent characters engaged in a prolonged performance in virtual worlds their use also may motivate emerging social mores politics and ideologies this paper argues that such avatar species create new social interactions and modes of communication that may signal interesting directions for future research\n",
      "  (id = 363, with a score of 0.961861789226532)\n",
      "\n",
      "emergent game formats such as machinima that use game worlds as expressive 3d performance spaces have new expressive powers with an increase of the quality of their underlying graphic and animation systems nevertheless they still lack intuitive control mechanisms set direction and acting are limited by tools that were designed to create and play video games rather than produce expressive performance pieces these tools do a poor job of capturing the performative expression that characterizes more mature media such as film tangible interfaces can help open up the game systems for more intuitive character control needed for a greater level of expression in the digital real time world the tui3d project tangible user interfaces for real time 3d addresses production and performative challenges involved in creating machinima through the development of tangible interfaces for controlling 3d virtual actors and environments in real time in this paper we present the design and implementation of a tangible puppet prototype for virtual character control in the unreal game engine and discuss initial user feedback\n",
      "  (id = 92, with a score of 0.9612609148025513)\n",
      "\n",
      "in this paper we present a model that describes how design disciplines interact with each other in mulltidisciplinary game design we base this model on experience from the design of a computer augmented card game mytheme our purpose is to show how hardware software and game rules interact with and affect each other during the design process of games that are computer augmented the model revolves around a flexible core of design requirements suitable for multidisciplinary design projects this model is an adaptation of the classic iterative design model helping to explain clashes between design areas and aid focus shifting from one design discipline to another\n",
      "  (id = 277, with a score of 0.956291913986206)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_idx = 397\n",
    "compare = 3\n",
    "\n",
    "compare = min( compare, 10 )\n",
    "print( 'TEST ABSTRACT:\\n' + get_paper_abstract(test_idx) )\n",
    "print( '\\n  SIMILAR TO:\\n' )\n",
    "for idx_score in model.docvecs.most_similar( test_idx )[:compare]:\n",
    "    print( get_paper_abstract(idx_score[0]) )\n",
    "    print( '  (id = '+ str(idx_score[0]) + ', with a score of '+ str(idx_score[1]) +')\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_recommendations( recs, acts ):\n",
    "    \"\"\" recs = list of doc IDs as the recommended references/citations\n",
    "        acts = list of doc IDs as the actual references/citations\n",
    "        NOTE: the indices are given as the 'docs' indices, which then refer to actual doc IDs\n",
    "    \"\"\"\n",
    "    matches = 0\n",
    "    nomatches = 0\n",
    "    for rec in recs:\n",
    "        if rec in acts:\n",
    "            matches += 1\n",
    "        else:\n",
    "            nomatches += 1\n",
    "    return matches, nomatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "papers = pd.read_csv( '../../data-kw.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_references( index ):\n",
    "    \"\"\" Given a 'docs' index, get the respective document's references\n",
    "    \"\"\"\n",
    "    if index >= arXiv_start:\n",
    "        print( \"DEBUG: WE DON'T HAVE REFERENCES FOR THE ARXIV PAPERS\" )\n",
    "        return []\n",
    "    refs = list( papers[papers['INDEX'] == tag_map[index]]['REF_ID'] )\n",
    "    if ';' in refs[0]:\n",
    "        refs = refs[0].split(';')\n",
    "    refs = list( map(int, refs) )\n",
    "    return refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_recommendations( index ):\n",
    "    # TODO: Update this with infer_vector stuff\n",
    "    #       and better logic in general.\n",
    "    #doc_ids = [ tag_map[index] for index, _ in model.docvecs.most_similar( index) ]\n",
    "    \n",
    "    most_sim = 10\n",
    "    vec = model.infer_vector( get_paper_abstract(index).split() )\n",
    "    # Code you'll find on nycdatascience.com:\n",
    "    cossims = list( map( lambda v: cossim(vec, v), model.docvecs ) )\n",
    "    sim_ids = argmaxn( cossims, most_sim )\n",
    "    #for i in range(most_sim):\n",
    "        #print( sim_ids[i], cossims[sim_ids[i]] )\n",
    "    doc_ids = [ tag_map[i] for i in sim_ids ]\n",
    "    return doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 9997\n"
     ]
    }
   ],
   "source": [
    "tot_matches = 0\n",
    "tot_nomatches = 0\n",
    "for test in range(1000):\n",
    "    recs = get_recommendations( test )\n",
    "    refs = get_references( test )\n",
    "    matches, nomatches = check_recommendations( recs, refs )\n",
    "    tot_matches += matches\n",
    "    tot_nomatches += nomatches\n",
    "print( str(tot_matches) + \" \" + str(tot_nomatches) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Code you'll find on nycdatascience.com:\n",
    "def cossim( v1, v2 ):\n",
    "    return np.dot(v1, v2) / np.sqrt( np.dot(v1, v1) ) / np.sqrt( np.dot(v2, v2) )\n",
    "def argmaxn( l, n ):\n",
    "    l_copy = list(l)\n",
    "    args = []\n",
    "    for i in range(n):\n",
    "        arg = np.argmax(l_copy)\n",
    "        args.append(arg)\n",
    "        l_copy[arg] = -float('inf')\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code you'll find on that other website:\n",
    "import lda\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# TODO: max_features was initially 10000\n",
    "cvectorizer = CountVectorizer( min_df=4, max_features=5000, stop_words='english' )\n",
    "abstracts = [ ' '.join( x.words ) for x in docs ]\n",
    "cvz = cvectorizer.fit_transform( abstracts )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero row in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE LDA\n"
     ]
    }
   ],
   "source": [
    "n_topics = 20\n",
    "n_iter = 2000\n",
    "lda_model = lda.LDA( n_topics=n_topics, n_iter=n_iter )\n",
    "print( \"DONE LDA\" )\n",
    "X_topics = lda_model.fit_transform( cvz )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: user users interaction paper human interface virtual information\n",
      "Topic 1: image method images based using paper object proposed\n",
      "Topic 2: graph problem algorithm time graphs number log set\n",
      "Topic 3: algorithm problem algorithms problems optimization search solution based\n",
      "Topic 4: network networks nodes routing wireless traffic sensor performance\n",
      "Topic 5: data query database queries mining databases paper xml\n",
      "Topic 6: model software design language systems paper based approach\n",
      "Topic 7: security protocol key scheme based protocols attacks paper\n",
      "Topic 8: functions codes quantum function linear complexity number finite\n",
      "Topic 9: logic language paper semantics systems model theory based\n",
      "Topic 10: model network networks models social time analysis structure\n",
      "Topic 11: learning data classification based method methods clustering proposed\n",
      "Topic 12: method model distribution matrix estimation algorithm linear probability\n",
      "Topic 13: service services web systems applications distributed based paper\n",
      "Topic 14: performance memory time parallel data cache execution processor\n",
      "Topic 15: channel rate coding capacity interference information channels scheme\n",
      "Topic 16: power design test energy paper based fault circuit\n",
      "Topic 17: research paper software development systems design knowledge study\n",
      "Topic 18: agent agents game decision games model multi strategies\n",
      "Topic 19: information web based search text semantic retrieval documents\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 8\n",
    "topic_summaries = []\n",
    "\n",
    "topic_word = lda_model.topic_word_\n",
    "vocab = cvectorizer.get_feature_names()\n",
    "for i, topic_dist in enumerate( topic_word ):\n",
    "    topic_words = np.array( vocab )[ np.argsort(topic_dist) ][ : -(n_top_words+1):-1 ]\n",
    "    topic_summaries.append( ' '.join( topic_words ) )\n",
    "    print( 'Topic {}: {}'.format(i, ' '.join( topic_words) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('clustering', 0.3813590407371521),\n",
       " ('ranking', 0.37842971086502075),\n",
       " ('ensemble', 0.3661080598831177),\n",
       " ('fusion', 0.3224477767944336),\n",
       " ('classifiers', 0.32240602374076843),\n",
       " ('inference', 0.30264925956726074),\n",
       " ('clusterings', 0.2996658682823181),\n",
       " ('pruning', 0.2980048656463623),\n",
       " ('boosting', 0.2952820360660553),\n",
       " ('encoding', 0.293839693069458)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar( positive=['classification'], negative=['recognition'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15382886432060081"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_similarity( ['human', 'chess'], ['deep', 'blue'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# When checking user's input, (if necessary) see if the keywords are present in the model or not"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
